{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jnN2Rv82mfCS","-kxPXmJPydA0"],"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61446,"databundleVersionId":6962461,"sourceType":"competition"},{"sourceId":1807973,"sourceType":"datasetVersion","datasetId":1074109}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup and Package Installation","metadata":{"id":"p7dNrZe8HEKk"}},{"cell_type":"code","source":"!pip install -q segmentation_models_pytorch python-dotenv","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:34:39.682315Z","iopub.execute_input":"2024-02-07T04:34:39.682711Z","iopub.status.idle":"2024-02-07T04:35:05.102416Z","shell.execute_reply.started":"2024-02-07T04:34:39.682682Z","shell.execute_reply":"2024-02-07T04:35:05.101125Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Set up Torch Hub Checkpoints for SE-Net","metadata":{}},{"cell_type":"code","source":"# Remove the existing checkpoints directory and create a new directory for torch hub checkpoints\n!rm -r /root/.cache/torch/hub/checkpoints/\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n# Copy pre-trained SE-Net weights to checkpoints directory\n!cp /kaggle/input/se-net-pretrained-imagenet-weights/* /root/.cache/torch/hub/checkpoints/","metadata":{"id":"nbXT3MLym0g0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a827df2d-57bb-434f-b588-3bace5e4baf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"rm: cannot remove '/root/.cache/torch/hub/checkpoints/': No such file or directory\n"}]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch as tc\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nimport os,sys,cv2\nfrom torch.cuda.amp import autocast # autocast module for automatic mixed-precision training\nimport matplotlib.pyplot as plt\nimport albumentations as A\nimport segmentation_models_pytorch as smp\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.parallel import DataParallel # DataParallel for parallel training\nfrom glob import glob\nimport random","metadata":{"id":"kV2NrpJgEMqn","execution":{"iopub.status.busy":"2024-02-07T04:35:05.105218Z","iopub.execute_input":"2024-02-07T04:35:05.105813Z","iopub.status.idle":"2024-02-07T04:35:12.242742Z","shell.execute_reply.started":"2024-02-07T04:35:05.105759Z","shell.execute_reply":"2024-02-07T04:35:12.241676Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Config for Model training","metadata":{}},{"cell_type":"code","source":"class CFG:\n    # Set random seed for reproducibility\n    seed = 42\n    \n    # Model-related configurations\n    target_size = 1\n    model_name = 'Unet'\n    backbone = 'se_resnext50_32x4d'\n    in_chans = 1\n    \n    tile_size = 512  # Size of the tiles used for input image cropping\n    stride = tile_size // 2  # Stride for overlapping tiles during image processing\n\n    # Training-related configurations\n    image_size = 512\n    input_size = 512\n    train_batch_size = 24\n    valid_batch_size = train_batch_size * 2\n    epochs = 20\n    lr = 8e-5\n    chopping_percentile = 1e-3\n    rotate_p = 0.5\n    \n    # Augmentation configurations\n    train_aug_list = [\n        A.Rotate(limit=270, p=rotate_p),\n        A.RandomScale(scale_limit=(0.8, 1.25), interpolation=cv2.INTER_CUBIC, p=0.2),\n        A.RandomCrop(input_size, input_size, p=1),\n        A.Flip(p=0.5),\n        A.RandomGamma(p=rotate_p * 2 / 3),\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.2),\n        A.GaussianBlur(p=0.1),\n        A.MotionBlur(p=0.05),\n        A.GridDistortion(num_steps=5, distort_limit=0.5, p=0.1),\n        A.ElasticTransform(p=0.05, border_mode=cv2.BORDER_REFLECT_101, alpha_affine=4, sigma=6.0, alpha=120),\n        ToTensorV2(transpose_mask=True),\n    ]\n    train_aug = A.Compose(train_aug_list)\n    \n    valid_aug_list = [\n        ToTensorV2(transpose_mask=True),\n    ]\n    valid_aug = A.Compose(valid_aug_list)\n    \n\ndef set_seed(seed):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    tc.manual_seed(seed)\n    tc.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    tc.backends.cudnn.deterministic = True\n    tc.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nset_seed(CFG.seed)\nprint(f\"Seed set for reproducibility: {CFG.seed}\")","metadata":{"id":"HYM9N_YVESjc","execution":{"iopub.status.busy":"2024-02-07T04:35:12.244181Z","iopub.execute_input":"2024-02-07T04:35:12.245118Z","iopub.status.idle":"2024-02-07T04:35:12.264259Z","shell.execute_reply.started":"2024-02-07T04:35:12.245084Z","shell.execute_reply":"2024-02-07T04:35:12.262424Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Seed set for reproducibility: 42\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"le8Ww99CG8pn"}},{"cell_type":"code","source":"class SegmentationModel(nn.Module):\n    def __init__(self, CFG, weight=None):\n        super().__init__()\n        \n        # Create the segmentation model using Unet architecture\n        self.model = smp.Unet(\n            encoder_name=CFG.backbone,\n            encoder_weights=weight,\n            in_channels=CFG.in_chans,\n            classes=CFG.target_size,\n            activation=None,\n        )\n\n    def forward(self, image):\n        \"\"\"\n        Forward pass of the model without applying activation.\n        \"\"\"\n        output = self.model(image)\n        # output = output.squeeze(-1) \n        return output[:, 0]  # .sigmoid()\n\n\ndef build_model(weight=\"imagenet\"):\n    \"\"\"\n    Build and return the SegmentationModel.\n    \"\"\"\n    from dotenv import load_dotenv\n    load_dotenv()\n\n    print('model_name', CFG.model_name)\n    print('backbone', CFG.backbone)\n\n    model = SegmentationModel(CFG, weight)\n\n    return model.cuda()","metadata":{"id":"VaXxVSIuEikS","execution":{"iopub.status.busy":"2024-02-07T04:35:12.266885Z","iopub.execute_input":"2024-02-07T04:35:12.267285Z","iopub.status.idle":"2024-02-07T04:35:12.278560Z","shell.execute_reply.started":"2024-02-07T04:35:12.267232Z","shell.execute_reply":"2024-02-07T04:35:12.277551Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Additional Functions","metadata":{"id":"vG-07mM5HNj0"}},{"cell_type":"code","source":"def min_max_normalization(x:tc.Tensor)->tc.Tensor:\n    \"\"\"input.shape=(batch,f1,...)\"\"\"\n    shape=x.shape\n    if x.ndim>2:\n        x=x.reshape(x.shape[0],-1)\n\n    min_=x.min(dim=-1,keepdim=True)[0]\n    max_=x.max(dim=-1,keepdim=True)[0]\n    if min_.mean()==0 and max_.mean()==1:\n        return x.reshape(shape)\n\n    x=(x-min_)/(max_-min_+1e-9)\n    return x.reshape(shape)\n\ndef norm_with_clip(x:tc.Tensor,smooth=1e-5):\n    dim=list(range(1,x.ndim))\n    mean=x.mean(dim=dim,keepdim=True)\n    std=x.std(dim=dim,keepdim=True)\n    x=(x-mean)/(std+smooth)\n    x[x>5]=(x[x>5]-5)*1e-3 +5\n    x[x<-3]=(x[x<-3]+3)*1e-3-3\n    return x\n\ndef add_noise(x:tc.Tensor,max_randn_rate=0.1,randn_rate=None,x_already_normed=False):\n    \"\"\"input.shape=(batch,f1,f2,...) output's var will be normalizate  \"\"\"\n    ndim=x.ndim-1\n    if x_already_normed:\n        x_std=tc.ones([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n        x_mean=tc.zeros([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n    else:\n        dim=list(range(1,x.ndim))\n        x_std=x.std(dim=dim,keepdim=True)\n        x_mean=x.mean(dim=dim,keepdim=True)\n    if randn_rate is None:\n        randn_rate=max_randn_rate*np.random.rand()*tc.rand(x_mean.shape,device=x.device,dtype=x.dtype)\n    cache=(x_std**2+(x_std*randn_rate)**2)**0.5\n    #https://blog.csdn.net/chaosir1991/article/details/106960408\n\n    return (x-x_mean+tc.randn(size=x.shape,device=x.device,dtype=x.dtype)*randn_rate*x_std)/(cache+1e-7)","metadata":{"id":"xV9SJyWPHQcF","execution":{"iopub.status.busy":"2024-02-07T04:35:12.279981Z","iopub.execute_input":"2024-02-07T04:35:12.281269Z","iopub.status.idle":"2024-02-07T04:35:12.299822Z","shell.execute_reply.started":"2024-02-07T04:35:12.281237Z","shell.execute_reply":"2024-02-07T04:35:12.298388Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Custom Dataset","metadata":{"id":"s3mDBBuMHYz6"}},{"cell_type":"markdown","source":"### Why Padding?\n**Handling Non-divisible Dimensions:**\n\n- Padding is crucial for images with dimensions not divisible by tile_size. It ensures consistency in processing and prevents issues when dividing images into tiles.\n\n- Applying padding ensures that each tile, generated during image processing, has a consistent size without any remainder. This is essential for uniformity in subsequent operations.\n\n\n*Example*:\n```\npad0 would be calculated as (16 - 30 % 16) % 16, resulting in 2.\npad1 would be calculated as (16 - 25 % 16) % 16, resulting in 7.\n```\n**Explanation**\n- `pad0` and `pad1` are calculated using double modulo.\n- The inner modulo checks if padding is needed (non-zero remainder).\n- The outer modulo limits the padding value to be less than self.tile_size, ensuring controlled padding.\n- Resulting in padded dimensions that are multiples of self.tile_size, maintaining processing stability.","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, paths, is_label, do_sort=True):\n        \"\"\"\n        Initialize the CustomDataset.\n\n        Args:\n            paths (list): List of file paths.\n            is_label (bool): Flag indicating if the dataset contains labels.\n            do_sort (bool, optional): Flag to sort file paths. Defaults to True.\n        \"\"\"\n        self.paths = paths\n        self.tile_size = CFG.tile_size\n        \n        # Sort paths if required\n        if do_sort:\n            self.paths.sort()\n        self.is_label = is_label\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        img = cv2.imread(self.paths[index], cv2.IMREAD_GRAYSCALE)\n        img = tc.from_numpy(img)\n\n        # Compute padding for both dimensions\n        pad0 = (self.tile_size - img.shape[0] % self.tile_size) % self.tile_size\n        pad1 = (self.tile_size - img.shape[1] % self.tile_size) % self.tile_size\n\n        # Apply replication padding\n        img = np.pad(img, [(0, pad0), (0, pad1)], mode='edge')\n        img = tc.from_numpy(img)\n\n        if self.is_label:\n            img = (img != 0).to(tc.uint8) * 255\n        else:\n            img = img.to(tc.uint8)\n\n        return img","metadata":{"id":"DlxRJDtKHaQX","execution":{"iopub.status.busy":"2024-02-07T04:35:12.301290Z","iopub.execute_input":"2024-02-07T04:35:12.301659Z","iopub.status.idle":"2024-02-07T04:35:12.316943Z","shell.execute_reply.started":"2024-02-07T04:35:12.301631Z","shell.execute_reply":"2024-02-07T04:35:12.315694Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Load data with clipping","metadata":{}},{"cell_type":"markdown","source":"- This function loads data using a `CustomDataset`, then performs thresholding and normalization on the data if it is not a label.\n- Clip extreme values in the array, both the sides","metadata":{}},{"cell_type":"code","source":"def load_data(paths, is_label=False, do_sort=True):\n    data_loader = CustomDataset(paths, is_label, do_sort)\n    data_loader = DataLoader(data_loader, batch_size=16, num_workers=0)\n    x = tc.cat([batch for batch in tqdm(data_loader)], dim=0)\n    if not is_label:\n        TH = x.reshape(-1).numpy()\n        index = -int(len(TH) * CFG.chopping_percentile)\n        TH: int = np.partition(TH, index)[index]\n        x[x > TH] = int(TH)\n\n        TH = x.reshape(-1).numpy()\n        index = -int(len(TH) * CFG.chopping_percentile)\n        TH: int = np.partition(TH, -index)[-index]\n        x[x < TH] = int(TH)\n\n        x = (min_max_normalization(x.to(tc.float16)[None])[0]*255).to(tc.uint8)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:35:12.318487Z","iopub.execute_input":"2024-02-07T04:35:12.318964Z","iopub.status.idle":"2024-02-07T04:35:12.334549Z","shell.execute_reply.started":"2024-02-07T04:35:12.318935Z","shell.execute_reply":"2024-02-07T04:35:12.333657Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Loss Functions","metadata":{"id":"Bbt19bVKHqEM"}},{"cell_type":"markdown","source":"### Dice Loss PyTorch","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n\n        # comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = inputs.sigmoid()\n\n        # flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n\n        intersection = (inputs * targets).sum()\n        dice = (2.*intersection + smooth) / \\\n            (inputs.sum() + targets.sum() + smooth)\n\n        return 1 - dice","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:35:12.335976Z","iopub.execute_input":"2024-02-07T04:35:12.336907Z","iopub.status.idle":"2024-02-07T04:35:12.349420Z","shell.execute_reply.started":"2024-02-07T04:35:12.336867Z","shell.execute_reply":"2024-02-07T04:35:12.348309Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Composite loss functions","metadata":{}},{"cell_type":"markdown","source":"**The function `select_loss_function(epoch)` dynamically adapts the loss function based on the current training epoch.**\n- During early epochs (<= 15), Dice loss is prioritized for balanced training.\n- In intermediate epochs (15 < epoch <= 17), the model transitions to Balanced BCE-Lovasz Loss for accuracy.\n- Beyond epoch 17, Weighted BCE-Lovasz-Tversky Loss is used for reducing False positives.","metadata":{}},{"cell_type":"code","source":"bce_loss = nn.BCEWithLogitsLoss()\nlovasz_loss = smp.losses.LovaszLoss(mode='binary', per_image=False)\ntversky_loss = smp.losses.TverskyLoss(mode='binary', log_loss=False, from_logits=True)\ndice_loss = DiceLoss()\n\n\ndef balanced_bce_lovasz_loss(output, target):\n    return 0.5 * bce_loss(output, target) + 0.5 * lovasz_loss(output, target)\n\ndef weighted_bce_lovasz_tversky_loss(output, target):\n    return 0.25 * bce_loss(output, target) + 0.25 * lovasz_loss(output, target) + 0.5 * tversky_loss(output, target)\n\n# Define a function to get the appropriate loss based on the epoch\ndef select_loss_function(epoch):\n    if epoch <= 15:\n        return dice_loss\n    elif 15 < epoch <= 17:\n        return balanced_bce_lovasz_loss\n    else:\n        return weighted_bce_lovasz_tversky_loss","metadata":{"id":"mdNkejz7HplZ","execution":{"iopub.status.busy":"2024-02-07T04:35:12.350908Z","iopub.execute_input":"2024-02-07T04:35:12.352125Z","iopub.status.idle":"2024-02-07T04:35:12.367231Z","shell.execute_reply.started":"2024-02-07T04:35:12.352077Z","shell.execute_reply":"2024-02-07T04:35:12.366304Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Scores","metadata":{}},{"cell_type":"code","source":"def dice_coef(y_pred: tc.Tensor, y_true: tc.Tensor, thr=0.5, dim=(-1, -2), epsilon=0.001):\n    y_pred = y_pred.sigmoid()\n    y_true = y_true.to(tc.float32)\n    y_pred = (y_pred > thr).to(tc.float32)\n    \n    intersection = (y_true * y_pred).sum(dim=dim)\n    denominator = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    \n    dice = ((2 * intersection + epsilon) / (denominator + epsilon)).mean()\n    \n    return dice","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:35:12.370662Z","iopub.execute_input":"2024-02-07T04:35:12.371807Z","iopub.status.idle":"2024-02-07T04:35:12.380031Z","shell.execute_reply.started":"2024-02-07T04:35:12.371768Z","shell.execute_reply":"2024-02-07T04:35:12.379048Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Function to Load Training Dataset","metadata":{"id":"8sUoTsr7Hv-7"}},{"cell_type":"markdown","source":"**What is Tiling?**\n- Tiling involves breaking down a large image into smaller, non-overlapping or overlapping, tiles.\n- The stride determines the step size for moving through the image to extract each subsequent tile.\n\n**Why i used Tiling?**\n- **Edge Consistency:** Overlapping tiles maintain contextual information at edges, ensuring accurate segmentation by preventing the model from overlooking crucial details near tile borders.\n\n- **Adaptation to Varied Resolution:** Tiling allows segmentation models to handle images with diverse resolutions. This adaptability ensures effective segmentation across different scales, optimizing performance in varied scenarios.","metadata":{}},{"cell_type":"code","source":"def load_train_dataset(x: list, y: list):\n    image_size = CFG.image_size\n    in_chans = CFG.in_chans\n\n    train_x = []\n    train_y = []\n\n    for i in range(len(x)):\n        print('dataset ', i)\n        x_data = x[i]\n        y_data = y[i]\n\n        for index in range(x_data.shape[0] - in_chans + 1):\n            x_slice = x_data[index:index+in_chans, :, :]\n            y_slice = y_data[index:index+in_chans, :, :]\n\n            x1_list = list(range(0, x_slice.shape[2] - CFG.tile_size + 1, CFG.stride))\n            y1_list = list(range(0, x_slice.shape[1] - CFG.tile_size + 1, CFG.stride))\n\n            for y1 in y1_list:\n                for x1 in x1_list:\n                    y2 = y1 + CFG.tile_size\n                    x2 = x1 + CFG.tile_size\n                    tile_x = x_slice[:, y1:y2, x1:x2]\n                    tile_y = y_slice[:, y1:y2, x1:x2]\n\n                    train_x.append(tile_x)\n                    train_y.append(tile_y)\n\n    return train_x, train_y","metadata":{"id":"HJdveYhce7RK","execution":{"iopub.status.busy":"2024-02-07T04:35:12.381508Z","iopub.execute_input":"2024-02-07T04:35:12.383542Z","iopub.status.idle":"2024-02-07T04:35:12.396528Z","shell.execute_reply.started":"2024-02-07T04:35:12.383493Z","shell.execute_reply":"2024-02-07T04:35:12.395107Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class with Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"### Train Dataset Class with Data Augmentation and Tiled crop","metadata":{}},{"cell_type":"code","source":"class Custom_Train_Dataset(Dataset):\n    def __init__(self, x: list, y: list, arg=False):\n        super(Kaggld_Dataset, self).__init__()\n        self.x = x  \n        self.y = y \n        self.in_chans = CFG.in_chans\n        self.arg = arg\n        if arg:\n            self.transform = CFG.train_aug\n        else:\n            self.transform = CFG.valid_aug\n\n    def __len__(self) -> int:\n        return len(self.x)\n\n    def __getitem__(self, index):\n        x = self.x[index]\n        y = self.y[index]\n\n        # Transform\n        data = self.transform(image=x.numpy().transpose(1, 2, 0), mask=y.numpy().squeeze(0))\n\n        x = data['image']\n        y = data['mask'] >= 127\n\n        if self.arg:\n            i=np.random.randint(4)\n\n            x=x.rot90(i,dims=(1,2))\n            y=y.rot90(i,dims=(0,1))\n\n            for i in range(2):\n                if np.random.randint(2):\n                    x=x.flip(dims=(i,))\n                    if i>=1:\n                        y=y.flip(dims=(i-1,))\n\n        return x, y  # (uint8, uint8)","metadata":{"id":"Gl9B_cChHxMO","execution":{"iopub.status.busy":"2024-02-07T04:35:12.398217Z","iopub.execute_input":"2024-02-07T04:35:12.398903Z","iopub.status.idle":"2024-02-07T04:35:12.415975Z","shell.execute_reply.started":"2024-02-07T04:35:12.398853Z","shell.execute_reply":"2024-02-07T04:35:12.414617Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Validation Dataset Class with Center Crop","metadata":{}},{"cell_type":"code","source":"class Custom_Val_Dataset(Dataset):\n    def __init__(self,x:list,y:list):\n        super(Dataset,self).__init__()\n        self.x=x#list[(C,H,W),...]\n        self.y=y#list[(C,H,W),...]\n        self.image_size=CFG.image_size\n        self.in_chans=CFG.in_chans\n        self.transform=CFG.valid_aug\n\n    def __len__(self) -> int:\n        return sum([y.shape[0]-self.in_chans for y in self.y])\n\n    def __getitem__(self,index):\n        i=0\n        for x in self.x:\n            if index>x.shape[0]-self.in_chans:\n                index-=x.shape[0]-self.in_chans\n                i+=1\n            else:\n                break\n        x=self.x[i]\n        y=self.y[i]\n\n        x_index= (x.shape[1]-self.image_size)//2\n        y_index= (x.shape[2]-self.image_size)//2 \n        x=x[index:index+self.in_chans, x_index:x_index+self.image_size, y_index:y_index+self.image_size]\n        y=y[index+self.in_chans//2, x_index:x_index+self.image_size, y_index:y_index+self.image_size]\n        data = self.transform(image=x.numpy().transpose(1,2,0), mask=y.numpy())\n\n        x = data['image']\n        y = data['mask']>=127\n\n        return x,y#(uint8,uint8)","metadata":{"id":"bvtmClrSpm3c","execution":{"iopub.status.busy":"2024-02-07T04:35:12.417455Z","iopub.execute_input":"2024-02-07T04:35:12.418328Z","iopub.status.idle":"2024-02-07T04:35:12.436137Z","shell.execute_reply.started":"2024-02-07T04:35:12.418281Z","shell.execute_reply":"2024-02-07T04:35:12.434808Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{"id":"VwLFHs1cIBZE"}},{"cell_type":"markdown","source":"*Uncomment the kidney 2 loading section if using kidney 2 for training*","metadata":{}},{"cell_type":"code","source":"# Load kidney 1 dense data\ntrain_x = []\ntrain_y = []\nroot_path = \"/kaggle/input/blood-vessel-segmentation/\"\npaths = [root_path + \"/train/kidney_1_dense\"]\n\nfor i, path in enumerate(paths):\n    if path == root_path + \"/train/kidney_3_dense\":\n        continue\n    print('Loading kidney 1 images')\n    x = load_data(glob(f\"{path}/images/*.tif\"), is_label=False)\n    print('Loading kidney 1 labels')\n    y = load_data(glob(f\"{path}/labels/*.tif\"), is_label=True)\n    train_x.append(x)\n    train_y.append(y)\n\n    # Augmentation\n    train_x.append(x.permute(1, 2, 0))\n    train_y.append(y.permute(1, 2, 0))\n    train_x.append(x.permute(2, 0, 1))\n    train_y.append(y.permute(2, 0, 1))\n\n# Load kidney 3 data\nkidney_3s_label = sorted(glob(root_path + \"/train/kidney_3_sparse/labels/*\"))\nkidney_3d_label = sorted(glob(root_path + \"/train/kidney_3_dense/labels/*\"))\nkidney_3d_img = sorted(glob(root_path + \"/train/kidney_3_sparse/images/*\"))\n\n# Loading kidney 3 images with specific exclusion from sparse\nkidney_3_label = kidney_3s_label[0: 496] + kidney_3d_label + kidney_3s_label[997:]\nkidney_3_img = kidney_3d_img\n\nprint('Loading kidney 3 images')\nkid3_x = load_data(kidney_3_img, is_label=False, do_sort=False)\nprint('Loading kidney 3 labels')\nkid3_y = load_data(kidney_3_label, is_label=True, do_sort=False)\ntrain_x.append(kid3_x)\ntrain_y.append(kid3_y)\n\ntrain_x.append(kid3_x.permute(1, 2, 0))\ntrain_y.append(kid3_y.permute(1, 2, 0))\ntrain_x.append(kid3_x.permute(2, 0, 1))\ntrain_y.append(kid3_y.permute(2, 0, 1))\n\n# Uncomment the following section if using kidney 2 for training\n# kid2_img = sorted(glob(root_path + \"/train/kidney_2/images/*\"))[900:]\n# kid2_label = sorted(glob(root_path + \"/train/kidney_2/labels/*\"))[900:]\n# print('Loading kidney 2 images')\n# kid2_x = load_data(kid2_img, is_label=False, do_sort=False)\n# print('Loading kidney 2 labels')\n# kid2_y = load_data(kid2_label, is_label=True, do_sort=False)\n# train_x.append(kid2_x)\n# train_y.append(kid2_y)\n# train_x.append(kid2_x.permute(1, 2, 0))\n# train_y.append(kid2_y.permute(1, 2, 0))\n# train_x.append(kid2_x.permute(2, 0, 1))\n# train_y.append(kid2_y.permute(2, 0, 1))\n\n# Load validation data from kidney 2\nval_img = sorted(glob(root_path + \"/train/kidney_2/images/*\"))[900:]\nval_label = sorted(glob(root_path + \"/train/kidney_2/labels/*\"))[900:]\n\nprint('Loading Validation data X')\nval_x = load_data(val_img, is_label=False, do_sort=False)\nprint('Loading Validation data y')\nval_y = load_data(val_label, is_label=True, do_sort=False)","metadata":{"id":"fytWebKqIEIS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9b4ab6c-1ec1-414e-90f6-ad6f5278eb65","execution":{"iopub.status.busy":"2024-02-07T04:43:06.170610Z","iopub.execute_input":"2024-02-07T04:43:06.171101Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loading kidney 1 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 143/143 [02:01<00:00,  1.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading kidney 1 labels\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 143/143 [01:08<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading kidney 3 images\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 52/65 [01:11<00:17,  1.34s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load padded images and mask**","metadata":{}},{"cell_type":"code","source":"train_image, train_mask = load_train_dataset(train_x, train_y)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ae3zis8SY_K","outputId":"a1fd8c9b-8701-40cb-deb5-5211b781f791"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"dataset  0\n\ndataset  1\n\ndataset  2\n\ndataset  3\n\ndataset  4\n\ndataset  5\n"}]},{"cell_type":"markdown","source":"## Scheduler","metadata":{}},{"cell_type":"markdown","source":"reference: https://github.com/ildoonet/pytorch-gradual-warmup-lr/blob/master/warmup_scheduler/scheduler.py","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nimport torch\n\nclass GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        total_epoch: target learning rate is reached at total_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.total_epoch = total_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n\n    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n        if self.last_epoch <= self.total_epoch:\n            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n                param_group['lr'] = lr\n        else:\n            if epoch is None:\n                self.after_scheduler.step(metrics, None)\n            else:\n                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n\n    def step(self, epoch=None, metrics=None):\n        if type(self.after_scheduler) != ReduceLROnPlateau:\n            if self.finished and self.after_scheduler:\n                if epoch is None:\n                    self.after_scheduler.step(None)\n                else:\n                    self.after_scheduler.step(epoch - self.total_epoch)\n                self._last_lr = self.after_scheduler.get_last_lr()\n            else:\n                return super(GradualWarmupScheduler, self).step(epoch)\n        else:\n            self.step_ReduceLROnPlateau(metrics, epoch)","metadata":{"id":"8NQPHvEfsQB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup Training","metadata":{"id":"imDuuX7re2mN"}},{"cell_type":"code","source":"tc.backends.cudnn.enabled = True\ntc.backends.cudnn.benchmark = True\n\ntrain_dataset = Kaggld_Dataset(train_image, train_mask, arg=True)\ntrain_dataset = DataLoader(train_dataset, batch_size=CFG.train_batch_size, num_workers=2, shuffle=True, pin_memory=True)\n# tc.save(train_dataset, '/content/drive/MyDrive/001_Projects/008_SenNet/SenNet/dataloader/DataLoader_train_k1k3_512.pth')\n\nval_dataset = Val_Dataset([val_x], [val_y])\nval_dataset = DataLoader(val_dataset, batch_size=CFG.valid_batch_size, num_workers=2, shuffle=False, pin_memory=True)\n# tc.save(val_dataset, '/content/drive/MyDrive/001_Projects/008_SenNet/SenNet/dataloader/DataLoader_val_k2_512.pth')\n\nmodel = build_model()\n# loss_fc = DiceLoss()\n# loss_fc = WeightedBCEandDiceLoss(alpha=1, beta=3)\noptimizer = tc.optim.AdamW(model.parameters(), lr=CFG.lr)\nscaler = tc.cuda.amp.GradScaler()\n","metadata":{"id":"Cs6jz37zeT0h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7197f37-8085-4eab-d6ac-60847d63191e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"model_name Unet\n\nbackbone se_resnext50_32x4d\n"}]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"vyBWtnzGe7P0"}},{"cell_type":"code","source":"import gc\ntc.cuda.empty_cache()\ngc.collect()\nwith tc.no_grad():\n    tc.cuda.empty_cache()","metadata":{"id":"mxpyzhtan8Yu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(predict, truth):\n    p = (predict > 0.5)\n    t = (truth > 0.5)\n    hit = (p * t).sum()\n    fp = (p * (1 - t)).sum()\n    t_sum = t.sum()\n    p_sum = p.sum()\n    return hit, fp, t_sum, p_sum","metadata":{"id":"sBtz6BSKypQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\nmodel.cuda()\n\nbest_val_score = float('-inf')\nbest_val_hit_rate = float('-inf')\nbest_val_false_positives = float('inf')\n\n# Training loop1\nfor epoch in range(CFG.epochs):\n    # Training phase\n    model.train()\n    loss_fc=get_loss(epoch)\n    train_time = tqdm(train_dataset, desc=f\"Epoch {epoch}\")\n    train_loss = 0\n    train_scores = 0\n    train_total_hit = 0\n    train_total_fp = 0\n    train_total_t_sum = 0\n    train_total_p_sum = 0\n\n    for i, (x, y) in enumerate(train_time):\n        x = x.to(device, dtype=tc.float32)\n        y = y.to(device, dtype=tc.float32)\n        x = norm_with_clip(x.reshape(-1, *x.shape[2:])).reshape(x.shape)\n        x = add_noise(x, max_randn_rate=0.5, x_already_normed=True)\n\n        with autocast():\n            pred = model(x)\n            # pred = pred.squeeze(1)\n            loss = loss_fc(pred, y)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        scheduler.step()\n        score = dice_coef(pred.detach(), y)\n        train_loss = (train_loss * i + loss.item()) / (i + 1)\n        train_scores = (train_scores * i + score) / (i + 1)\n\n        # Calculate training metrics\n        train_hit, train_fp, train_t_sum, train_p_sum = calculate_metrics((pred.detach() > 0.5).cpu().numpy(), y.cpu().numpy())\n        train_total_hit += train_hit\n        train_total_fp += train_fp\n        train_total_t_sum += train_t_sum\n        train_total_p_sum += train_p_sum\n\n        train_time.set_description(f\"epoch:{epoch}, train_loss:{train_loss:.4f}, train_score:{train_scores:.4f}, lr{optimizer.param_groups[0]['lr']:.4e}, train_hit_rate:{train_total_hit / train_total_t_sum:.4f}, train_false_positives:{train_total_fp / train_total_p_sum:.4f}\")\n        train_time.update()\n        del loss, pred\n\n    train_time.close()\n\n    # Validation phase\n    model.eval()\n    val_time = tqdm(val_dataset, desc=f\"Epoch {epoch}\")\n    val_loss = 0\n    val_scores = 0\n    val_total_hit = 0\n    val_total_fp = 0\n    val_total_t_sum = 0\n    val_total_p_sum = 0\n\n    for i, (x, y) in enumerate(val_time):\n        x = x.to(device, dtype=tc.float32)\n        y = y.to(device, dtype=tc.float32)\n        x = norm_with_clip(x.reshape(-1, *x.shape[2:])).reshape(x.shape)\n\n        with autocast():\n            with tc.no_grad():\n                pred = model(x)\n                # pred = pred.squeeze(1)\n                loss = loss_fc(pred, y)\n\n        score = dice_coef(pred.detach(), y)\n        val_loss = (val_loss * i + loss.item()) / (i + 1)\n        val_scores = (val_scores * i + score) / (i + 1)\n\n        # Calculate validation metrics\n        val_hit, val_fp, val_t_sum, val_p_sum = calculate_metrics((pred.detach() > 0.5).cpu().numpy(), y.cpu().numpy())\n        val_total_hit += val_hit\n        val_total_fp += val_fp\n        val_total_t_sum += val_t_sum\n        val_total_p_sum += val_p_sum\n\n        val_time.set_description(f\"epoch:{epoch}, val_loss:{val_loss:.4f}, val_score:{val_scores:.4f}, val_hit_rate:{val_total_hit / val_total_t_sum:.4f}, val_false_positives:{val_total_fp / val_total_p_sum:.4f}\")\n        val_time.update()\n\n    val_time.close()\n\n    # Update best model based on validation score\n    if val_scores > best_val_score:\n        best_val_score = val_scores\n        tc.save(model.state_dict(), f\"/SenNet/models/{CFG.backbone}_tile_best_val_score_model.pt\")\n\n    # Update best model based on validation false positives\n    val_false_positives_rate = val_total_fp / val_total_p_sum\n    if val_false_positives_rate < best_val_false_positives:\n        best_val_false_positives = val_false_positives_rate\n        tc.save(model.state_dict(), f\"/SenNet/SenNet/models/{CFG.backbone}_tile_best_false_positives_model.pt\")","metadata":{"id":"jA8tpFlfyfnq","colab":{"base_uri":"https://localhost:8080/","height":477},"outputId":"dd6081ab-3b05-405b-e9f5-53c636954958"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"epoch:0, train_loss:0.2891, train_score:0.8124, lr4.8210e-05, train_hit_rate:0.8000, train_false_positives:0.3553: 100%|██████████| 3598/3598 [40:17<00:00,  1.49it/s]\n\nepoch:0, val_loss:0.0730, val_score:0.9410, val_hit_rate:0.9651, val_false_positives:0.0091: 100%|██████████| 21/21 [00:21<00:00,  1.02s/it]\n\nepoch:1, train_loss:0.2089, train_score:0.8621, lr3.6697e-04, train_hit_rate:0.8688, train_false_positives:0.0662: 100%|██████████| 3598/3598 [39:52<00:00,  1.50it/s]\n\nepoch:1, val_loss:0.0663, val_score:0.9452, val_hit_rate:0.9698, val_false_positives:0.0095: 100%|██████████| 21/21 [00:12<00:00,  1.67it/s]\n\nepoch:2, train_loss:0.1893, train_score:0.8725, lr4.3303e-04, train_hit_rate:0.8831, train_false_positives:0.0629:  49%|████▊     | 1747/3598 [19:22<20:31,  1.50it/s]\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-5a1901bcf39f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":"tc.save(model.state_dict(), f\"SenNet/SenNet/models/{CFG.backbone}_tile_last_epoch.pt\")","metadata":{"id":"lDAjhYcXvbu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.state_dict()","metadata":{"id":"wmhQztJzqPLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ApLlo_JErdrB"},"execution_count":null,"outputs":[]}]}